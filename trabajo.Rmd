# Trabajo FID

El dataset utilizado ha sido https://www.kaggle.com/imdevskp/corona-virus-report?select=day_wise.csv

# Predicción supervisada

Para estas predicciones hemos usado un dataset extraido de kaggle (https://www.kaggle.com/imdevskp/corona-virus-report?select=day_wise.csv) que muestra los datos del covid a nivel mundial divididos por días.
Trataremos de predecir la estación del año a la que pertenece el dato, teniendo en cuenta los casos, las muertes y las recuperaciones diarias.

## CLASIFICACIÓN MEDIANTE ÁRBOL

Instalamos la librería rattle para poder hacer la predicción mediante árbol

```{r}
install.packages("rattle")
```

Cargamos el paquete
```{r}
library(rpart)
library(rpart.plot)
```

```{r}
data_arbol <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
colnames(data)
```

Ahora vamos a generar y a dibujar el árbol
```{r}
tree <- rpart(Seasson ~ New.cases + New.deaths + New.recovered, data_arbol, method="class")
rpart.plot(tree)
```
 
#TODO

Ahora vamos a cargar los datos de testeo para poder probar la capacidad de predicción del árbol generado
```{r}
test <- read.delim('datos/arbol_test.csv', sep=",", head = TRUE)
colnames(data)
```

Ahora vamos a predecir los valores del conjunto de test y obtener la matriz de confusión y acurracy
```{r}
pred <- predict(tree, test, type="class")
conf <- table(test$Confirmed, pred)
View(conf)

acc <- sum(diag(conf)) / sum(conf)
print(acc)
```
 
## Naive Bayes

Fuente: https://fervilber.github.io/Aprendizaje-supervisado-en-R/ingenuo.html

Naive Bayes es un modelo para predecir por probabilidad Bayesiana, de tal forma que clasifica el resultado en función de variables que a priori son independientes entre sí

Primero instalarmos el paquete naivebayes, además es necesario usar Rtools (https://cran.r-project.org/bin/windows/Rtools/)
```{r}
install.packages("naivebayes")
```

Después cargamos el paquete
```{r}
library(naivebayes)
```

Cargamos los datos
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
```

E introducimos los datos, es importante factorizar la propiedad de "Season" para que funcione la función
```{r}
nb <- naive_bayes(as.factor(Seasson) ~ New.cases + New.deaths + New.recovered, data)

plot(nb)
```

Ahora vamos a mostrar la tabla de probabilidades, para cada atributo se muestra el valor medio que tienen estos atributos para cada estación
```{r}
nb$tables
```
De esta forma, Naive bayes es capaz de calcular la probabilidad de que la muestra se trate de una estación u otra según el número de casos, muertes y recuperaciones diarias.

#TODO

Ahora vamos a comprobar que es capaz de predecir este modelo.

```{r}
data_test = data[1,]
data_test$New.cases = 1000
data_test$New.deaths = 2000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict

```
```{r}
data_test = data[1,]
data_test$New.cases = 500
data_test$New.deaths = 20000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict
```

## SVM - Máquinas de vectores de soporte 

Fuentes: https://www.diegocalvo.es/svm-maquinas-de-vectores-de-soporte-en-r/

Este método de clasificación se basa en la busqueda un hiperplano que separe de forma óptima a todos los puntos de una clase, clasificandolos.
El algoritmo de SVM tratará de buscar el hiperplano que tenga la máxima distancia posible con los puntos, de esta forma se podrá hacer una mejor clasificación de los datos, 
etiquetando cada dato dependiendo de en que lado del hiperplano se encuentra.

```{r}
library(e1071)
# Ejecución del modelo SVM
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)

dat = data.frame(x= data$New.cases, y = as.factor(data$Seasson))
modelo = svm(as.factor(Seasson) ~ ., data = data, kernel = "linear", cost = 10, scale = FALSE)

```

#TODO

Ahora mediante el dataset de testeo probaremos la capacidad de predicción de este modelo.
```{r}
prediccion <- predict(modelo,new=data) #Hay que inventar

(mc <- with(data,(table(prediccion,Seasson))))
```


# Predicción no supervisada

## 1. KMeans (https://rpubs.com/williamsurles/310847) (https://www.geeksforgeeks.org/clustering-in-r-programming)

La agrupación en R es una técnica de aprendizaje no supervisada en la que el conjunto de datos se divide en varios grupos llamados "clusters" en función de su similitud. Después de la segmentación de los datos se producen varios grupos de datos. Todos los objetos de un grupo comparten características comunes. Durante la extracción y el análisis de datos, la agrupación se utiliza para encontrar los conjuntos de datos similares.

K-Means es una técnica iterativa de agrupamiento duro que utiliza un algoritmo de aprendizaje no supervisado. En él, el número total de conglomerados es predefinido por el usuario, y en base a la similitud de cada punto de datos, los puntos de datos se agrupan. Este algoritmo también descubre el centroide del cúmulo.

Cargamos librerias
```{r}
library(ggplot2)
```
Cargamos y visualizamos los datos
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
head(data)

ggplot(data, aes(New.cases, Deaths, Recovered, color = Seasson)) + geom_point()

```
Interpretamos los datos. Podemos obsevar como hay 3 grupos. (1 = invierno, 2 = primavera,3 = verano)
```{r}
km.out <- kmeans(data[0:15], centers = 3, nstart = 20)
summary(km.out) # Inspect the result
plot(data, 
  col = km.out$cluster,
  main = "k-means with 3 clusters")
```
Necesitamos elegir k, el número de clusters. 
Hay un punto donde la curva SSE comienza a doblarse conocido como el punto del codo. Se cree que el valor x de este punto es un equilibrio razonable entre el error y el número de cúmulos. En nuestro caso, tras observar la gráfica podemos ver que es 3.
```{r}
wss <- 0
for (i in 1:15) {
  km.out <- kmeans(x = data[0:15], centers = i, nstar=20)
  wss[i] <- km.out$tot.withinss
}
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
```

```{r}
k <- 3
km <- kmeans(data[0:15], centers = k, nstart = 20)
plot(data,
     col = km$cluster,
     main = paste("k-means clustering with", k, "clusters"))

table(km$cluster, data$Seasson)
```
Visualizar el resultado:
```{r}
ggplot(data, aes(New.cases, Deaths, Recovered, color = km$cluster)) + geom_point()
```
## 2. Principal component analysis (https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#principal-component-analysis-pca)
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train_kmeans.csv', sep=",", head = TRUE)
data$Deaths <- NULL
data$Recovered <- NULL
data$Active <- NULL
data$New.cases <- NULL
data$New.deaths <- NULL
data$New.recovered <- NULL
data$Deaths...100.Cases <- NULL
data$Deaths...100.Recovered <- NULL
data$No..of.countries <- NULL
data$New.Cases.gt.5000 <- NULL
data$New.Deaths.gt.5000 <- NULL
data$New.Recover.gt.5000 <- NULL
data$Date <- NULL
data$Recovered...100.Cases <- NULL
head(data)

pairs(data[, -5], pch = 19)

# Ahora vamos a reducir las dimensiones
irispca <- prcomp(data[, -5])
summary(irispca)

# Un biplot presenta todos los puntos originales re-mapeados (rotados) a lo largo de las dos primeras PCs, así como las características originales como vectores a lo largo de las mismas PCs.

# Extraemos las desviaciones estándar del resultado del PCA para calcular las desviaciones, y luego obtengo el porcentaje y la varianza acumulada a lo largo de los PC.
biplot(irispca)
var <- irispca$sdev^2
(pve <- var/sum(var))
```