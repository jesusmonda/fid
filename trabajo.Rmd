# Trabajo FID

El dataset utilizado ha sido https://www.kaggle.com/imdevskp/corona-virus-report?select=day_wise.csv

# Predicción supervisada

Para estas predicciones hemos usado un dataset extraido de kaggle (https://www.kaggle.com/imdevskp/corona-virus-report?select=day_wise.csv) que muestra los datos del covid a nivel mundial divididos por días.
Trataremos de predecir la estación del año a la que pertenece el dato, teniendo en cuenta los casos, las muertes y las recuperaciones diarias.

## CLASIFICACIÓN MEDIANTE ÁRBOL

Instalamos la librería rattle para poder hacer la predicción mediante árbol

```{r}
install.packages("rattle")
```

Cargamos el paquete
```{r}
library(rpart)
library(rpart.plot)
```

```{r}
data_arbol <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
colnames(data)
```

Ahora vamos a generar y a dibujar el árbol
```{r}
tree <- rpart(Seasson ~ New.cases + New.deaths + New.recovered, data_arbol, method="class")
rpart.plot(tree)
```
 
#TODO

Ahora vamos a cargar los datos de testeo para poder probar la capacidad de predicción del árbol generado
```{r}
test <- read.delim('datos/arbol_test.csv', sep=",", head = TRUE)
colnames(data)
```

Ahora vamos a predecir los valores del conjunto de test y obtener la matriz de confusión y acurracy
```{r}
pred <- predict(tree, test, type="class")
conf <- table(test$Confirmed, pred)
View(conf)

acc <- sum(diag(conf)) / sum(conf)
print(acc)
```
 
## Naive Bayes

Fuente: https://fervilber.github.io/Aprendizaje-supervisado-en-R/ingenuo.html

Naive Bayes es un modelo para predecir por probabilidad Bayesiana, de tal forma que clasifica el resultado en función de variables que a priori son independientes entre sí

Primero instalarmos el paquete naivebayes, además es necesario usar Rtools (https://cran.r-project.org/bin/windows/Rtools/)
```{r}
install.packages("naivebayes")
```

```{r}
library(naivebayes)
```

```{r}
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
```

Mediante este método, vamos a calcular la probabilidad de que el dato haya sido tomado en una estación o en otra sea de una estación u otra dependiendo de los casos nuevos, el registro de muertes diario y el número de recuperados diario.

```{r}
nb <- naive_bayes(as.factor(Seasson) ~ New.cases + New.deaths + New.recovered, data)

plot(nb)
```

Ahora vamos a mostrar la tabla de probabilidades, para cada atributo se muestra el valor medio que tienen estos atributos para cada estación
```{r}
nb$tables
```
De esta forma, Naive bayes es capaz de calcular la probabilidad de que la muestra se trate de una estación u otra según el número de casos, muertes y recuperaciones diarias.

#TODO

```{r}
data_test = data[1,]
data_test$New.cases = 1000
data_test$New.deaths = 2000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict

```
```{r}
data_test = data[1,]
data_test$New.cases = 500
data_test$New.deaths = 20000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict
```

## SVM - Máquinas de vectores de soporte 

Fuentes: https://www.diegocalvo.es/svm-maquinas-de-vectores-de-soporte-en-r/

Este método de clasificación se basa en la busqueda un hiperplano que separe de forma óptima a todos los puntos de una clase, clasificandolos.
El algoritmo de SVM tratará de buscar el hiperplano que tenga la máxima distancia posible con los puntos, de esta forma se podrá hacer una mejor clasificación de los datos, 
etiquetando cada dato dependiendo de en que lado del hiperplano se encuentra.

```{r}
library(e1071)
# Ejecución del modelo SVM
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)

dat = data.frame(x= data$New.cases, y = as.factor(data$Seasson))
modelo = svm(as.factor(Seasson) ~ ., data = data, kernel = "linear", cost = 10, scale = FALSE)

```

```{r}
prediccion <- predict(modelo,new=data) #Hay que inventar

(mc <- with(data,(table(prediccion,Seasson))))
```


# Predicción no supervisada

## 1. KMeans (https://rpubs.com/williamsurles/310847) (https://www.geeksforgeeks.org/clustering-in-r-programming)
```{r}
#La agrupación en R es una técnica de aprendizaje no supervisada en la que el conjunto de datos se divide en varios grupos llamados "clusters" en función de su similitud. Después de la segmentación de los datos se producen varios grupos de datos. Todos los objetos de un grupo comparten características comunes. Durante la extracción y el análisis de datos, la agrupación se utiliza para encontrar los conjuntos de datos similares.

#K-Means es una técnica iterativa de agrupamiento duro que utiliza un algoritmo de aprendizaje no supervisado. En él, el número total de conglomerados es predefinido por el usuario, y en base a la similitud de cada punto de datos, los puntos de datos se agrupan. Este algoritmo también descubre el centroide del cúmulo.

data <- read.delim('datos_nuevos_casos/regresion_train_kmeans.csv', sep=",", head = TRUE)
data$Deaths <- NULL
data$Recovered <- NULL
data$Active <- NULL
data$New.cases <- NULL
data$New.deaths <- NULL
data$New.recovered <- NULL
data$Deaths...100.Cases <- NULL
data$Deaths...100.Recovered <- NULL
data$No..of.countries <- NULL
data$New.Cases.gt.5000 <- NULL
data$New.Deaths.gt.5000 <- NULL
data$New.Recover.gt.5000 <- NULL
data$Date <- NULL
data$Recovered...100.Cases <- NULL
head(data)

# Interpetar los datos, podemos obsevar como hay claramente 3 grupos. (1 = invierno, 2 = primavera,3 = verano)
km.out <- kmeans(data, centers = 3, nstart = 20)
summary(km.out) # Inspect the result
plot(data, 
  col = km.out$cluster,
  main = "k-means with 3 clusters",
  xlab = "Confirmados", ylab = "Epoca")

# Determinar el numero de cluster
wss <- 0
for (i in 1:15) {
  km.out <- kmeans(x = data, centers = i, nstar=20)
  wss[i] <- km.out$tot.withinss
}
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")

# Hay un punto dulce donde la curva SSE comienza a doblarse conocido como el punto del codo. Se cree que el valor x de este punto es un equilibrio razonable entre el error y el número de cúmulos. En nuestro caso es 4

k <- 4
km <- kmeans(data, centers = k, nstart = 20, iter.max = 50)
plot(data,
     col = km$cluster,
     main = paste("k-means clustering with", k, "clusters"),
     xlab = "Confirmados", ylab = "Epoca")
```
## 2. Principal component analysis (https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#principal-component-analysis-pca)
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train_kmeans.csv', sep=",", head = TRUE)
data$Deaths <- NULL
data$Recovered <- NULL
data$Active <- NULL
data$New.cases <- NULL
data$New.deaths <- NULL
data$New.recovered <- NULL
data$Deaths...100.Cases <- NULL
data$Deaths...100.Recovered <- NULL
data$No..of.countries <- NULL
data$New.Cases.gt.5000 <- NULL
data$New.Deaths.gt.5000 <- NULL
data$New.Recover.gt.5000 <- NULL
data$Date <- NULL
data$Recovered...100.Cases <- NULL
head(data)

pairs(data[, -5], pch = 19)

# Ahora vamos a reducir las dimensiones
irispca <- prcomp(data[, -5])
summary(irispca)

# Un biplot presenta todos los puntos originales re-mapeados (rotados) a lo largo de las dos primeras PCs, así como las características originales como vectores a lo largo de las mismas PCs.

# Extraemos las desviaciones estándar del resultado del PCA para calcular las desviaciones, y luego obtengo el porcentaje y la varianza acumulada a lo largo de los PC.
biplot(irispca)
var <- irispca$sdev^2
(pve <- var/sum(var))
```