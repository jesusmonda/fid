# Trabajo FID

El dataset utilizado ha sido https://www.kaggle.com/imdevskp/corona-virus-report?select=day_wise.csv

# Predicción supervisada

## CLASIFICACIÓN MEDIANTE ÁRBOL

Instalamos la librería rattle

```{r}
install.packages("rattle")
```

```{r}
library(rpart)
library(rpart.plot)
```

Vamos a realizar un arbol de decisión para obtener los campos más importantes.
```{r}
data_arbol <- read.delim('datos/arbol_train.csv', sep=",", head = TRUE)
colnames(data)
```

Dibujamos el arbol y lo exportamos al fichero arbol.pdf, podemos observar como el campo más importante es la fecha del contagio.
```{r}
tree <- rpart(Confirmed ~ ., data_arbol, method="class", minsplit = 2)
rpart.plot(tree)
```



Cargamos los datos de test
```{r}
test <- read.delim('datos/arbol_test.csv', sep=",", head = TRUE)
colnames(data)
```

Ahora vamos a predecir los valores del conjunto de test y obtener la matriz de confusión y acurracy
```{r}
pred <- predict(tree, test, type="class")
conf <- table(test$Confirmed, pred)
View(conf)

acc <- sum(diag(conf)) / sum(conf)
print(acc)
```

## REGRESIÓN LINEAL
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
```

```{r}
regresion <- lm(New.cases~Date, data = data)
summary(regresion)
```

Podemos observar como el número de contagios aumenta por día
```{r}
plot(data$Date, data$New.cases, xlab='Date', ylab='New Cases')
abline(regresion)
```

Predecimos un día correspondiente al archivo datos/regresion_test.csv, por ejemplo el día 28/05/2020 en el cual hubo 119314 nuevos contagios
```{r}

# Primero calculamos la diferencia de días
mydates <- as.Date(c("2020-05-28", "2020-01-22"))
days_diff <- mydates[1] - mydates[2]
days_diff <- as.numeric(days_diff)
#Hay 127 días de diferencia


date <- data.frame(Date = days_diff)
predict(regresion, date)
```
Ha predicho 13188, por lo que ha habido un error de 12569.

## Naive Bayes

Naive Bayes es un modelo para predecir por probabilidad Bayesiana, de tal forma que clasifica el resultado en función de variables que a priori son independientes entre sí

Primero instalarmos el paquete naivebayes, además es necesario usar Rtools (https://cran.r-project.org/bin/windows/Rtools/)
```{r}
install.packages("naivebayes")
```

```{r}
library(naivebayes)
```

```{r}
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
```

Mediante este método, vamos a calcular la probabilidad de que el dato haya sido tomado en una estación o en otra sea de una estación u otra dependiendo de los casos nuevos, el registro de muertes diario y el número de recuperados diario.

```{r}
nb <- naive_bayes(as.factor(Seasson) ~ New.cases + New.deaths + New.recovered, data)

plot(nb)
```

Ahora vamos a mostrar la tabla de probabilidades, para cada atributo se muestra el valor medio que tienen estos atributos para cada estación
```{r}
nb$tables
```
De esta forma, Naive bayes es capaz de calcular la probabilidad de que la muestra se trate de una estación u otra según el número de casos, muertes y recuperaciones diarias.

```{r}
data_test = data[1,]
data_test$New.cases = 1000
data_test$New.deaths = 2000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict

```
```{r}
data_test = data[1,]
data_test$New.cases = 500
data_test$New.deaths = 20000
data_test$New.recovered = 5000
predict = predict(nb, data_test, type="prob")    
predict
```

## SVM - Máquinas de vectores de soporte (Probandose)

```{r}
library(e1071)

# Ejecución del modelo SVM
data <- read.delim('datos_nuevos_casos/regresion_train.csv', sep=",", head = TRUE)
modelo <- svm(as.factor(Seasson)~., data=data)

data_test = data[1,]
data_test$New.cases = 100
data_test$New.deaths = 2000
data_test$New.recovered = 500

# Predicción de los restantes
prediction <- predict(modelo,new=data_test)
prediction

plot(modelo)
```


# Predicción no supervisada

## 1. KMeans (https://rpubs.com/williamsurles/310847) (https://www.geeksforgeeks.org/clustering-in-r-programming)
```{r}
#La agrupación en R es una técnica de aprendizaje no supervisada en la que el conjunto de datos se divide en varios grupos llamados "clusters" en función de su similitud. Después de la segmentación de los datos se producen varios grupos de datos. Todos los objetos de un grupo comparten características comunes. Durante la extracción y el análisis de datos, la agrupación se utiliza para encontrar los conjuntos de datos similares.

#K-Means es una técnica iterativa de agrupamiento duro que utiliza un algoritmo de aprendizaje no supervisado. En él, el número total de conglomerados es predefinido por el usuario, y en base a la similitud de cada punto de datos, los puntos de datos se agrupan. Este algoritmo también descubre el centroide del cúmulo.

data <- read.delim('datos_nuevos_casos/regresion_train_kmeans.csv', sep=",", head = TRUE)
data$Deaths <- NULL
data$Recovered <- NULL
data$Active <- NULL
data$New.cases <- NULL
data$New.deaths <- NULL
data$New.recovered <- NULL
data$Deaths...100.Cases <- NULL
data$Deaths...100.Recovered <- NULL
data$No..of.countries <- NULL
data$New.Cases.gt.5000 <- NULL
data$New.Deaths.gt.5000 <- NULL
data$New.Recover.gt.5000 <- NULL
data$Date <- NULL
data$Recovered...100.Cases <- NULL
head(data)

# Interpetar los datos, podemos obsevar como hay claramente 3 grupos. (1 = invierno, 2 = primavera,3 = verano)
km.out <- kmeans(data, centers = 3, nstart = 20)
summary(km.out) # Inspect the result
plot(data, 
  col = km.out$cluster,
  main = "k-means with 3 clusters",
  xlab = "Confirmados", ylab = "Epoca")

# Determinar el numero de cluster
wss <- 0
for (i in 1:15) {
  km.out <- kmeans(x = data, centers = i, nstar=20)
  wss[i] <- km.out$tot.withinss
}
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")

# Hay un punto dulce donde la curva SSE comienza a doblarse conocido como el punto del codo. Se cree que el valor x de este punto es un equilibrio razonable entre el error y el número de cúmulos. En nuestro caso es 4

k <- 4
km <- kmeans(data, centers = k, nstart = 20, iter.max = 50)
plot(data,
     col = km$cluster,
     main = paste("k-means clustering with", k, "clusters"),
     xlab = "Confirmados", ylab = "Epoca")
```
## 2. Principal component analysis (https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#principal-component-analysis-pca)
```{r}
data <- read.delim('datos_nuevos_casos/regresion_train_kmeans.csv', sep=",", head = TRUE)
data$Deaths <- NULL
data$Recovered <- NULL
data$Active <- NULL
data$New.cases <- NULL
data$New.deaths <- NULL
data$New.recovered <- NULL
data$Deaths...100.Cases <- NULL
data$Deaths...100.Recovered <- NULL
data$No..of.countries <- NULL
data$New.Cases.gt.5000 <- NULL
data$New.Deaths.gt.5000 <- NULL
data$New.Recover.gt.5000 <- NULL
data$Date <- NULL
data$Recovered...100.Cases <- NULL
head(data)

pairs(data[, -5], pch = 19)

# Ahora vamos a reducir las dimensiones
irispca <- prcomp(data[, -5])
summary(irispca)

# Un biplot presenta todos los puntos originales re-mapeados (rotados) a lo largo de las dos primeras PCs, así como las características originales como vectores a lo largo de las mismas PCs.

# Extraemos las desviaciones estándar del resultado del PCA para calcular las desviaciones, y luego obtengo el porcentaje y la varianza acumulada a lo largo de los PC.
biplot(irispca)
var <- irispca$sdev^2
(pve <- var/sum(var))
```